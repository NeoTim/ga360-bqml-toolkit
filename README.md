# BQML Toolkit

The goal of this toolkit is to jumpstart the process of creating propensity
models in BQML. The toolkit provides templated SQL queries to generate feature
sets and models.

(TODO) - samchu@ - add more info on why clients would use these models

This is not meant to be an exhaustive solution or cover every use case. No two
models are the same, so you should tweak with your own timelines and business
case in mind.

For more on BQML, see the
[documentation](https://cloud.google.com/bigquery-ml/docs).

## Components

### Queries

The `sql` folder contains sample scripts, some of which are utilized in `create_and_run_queries.py`.
Most importantly, `create_daily_feature_set.sql` creates generalizable features at the user-level,
which are then used for predicting future behavior. 

### Python script

The Python script included will automate the process of reading in the base
feature set and model training SQL files, passing in the necessary parameters,
running the queries in BigQuery, and saving the results.

## Installation

### Pull in GA360 data

This repo solely utilizes data generated by the GA360 BigQuery Export.
If you are GA360 customer but do not have the export implemented, see [this page](https://support.google.com/analytics/answer/3416092)
for instructions.
Alternatively, you can use publicly available data by following directions [here](https://support.google.com/analytics/answer/7586738).

#### Modify the queries (optional)

(TODO) - samchu@ to add some info about how the queries could be modified (and
what the use cases for that would be)

### Running the Python script
If you want to run on Cloud Shell, click the button below to deploy. You will
need to run `gcloud init login` to make sure your credentials are up to date. 
[![Run on Google Cloud](https://deploy.cloud.run/button.svg)](https://deploy.cloud.run)

If you want to run in your own environment, you will need to set up
authentication. Follow [the steps outlined
here](https://cloud.google.com/docs/authentication/production#create_service_account) to create a service account and
save it in your environment. Then you will need to add these credentials as an
environment variable.

To run, you need a few pieces of information: * `project_id`: the ID of your GCP
project * `ga_dataset`: the BigQuery dataset where you have stored your GA
export data * `ga_table_prefix`: the prefix of the BigQuery table where you
stored your GA export data (This should be a set of partitioned tables. The
script is already set up to pass in a wildcard so that it will read in all the
partitioned tables)

Then, run this command in Cloud Shell: `python3 create_and_run_queries.py [project_id]
[ga_dataset]`

For more advanced usage you can modify the script (for example, to use custom
parameters). Simply edit the parameters at the beginning of the file.

### What next?

Now that you have your BQML model set up, the next step is activation. Check out
[Project Modem](https://github.com/google/modem) if you would like to create an
automated pipeline for activating the data from your model.

