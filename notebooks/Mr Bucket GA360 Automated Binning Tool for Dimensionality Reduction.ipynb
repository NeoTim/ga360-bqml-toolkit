{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Mr Bucket GA360 Automated Binning Tool for Dimensionality Reduction","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"mWJaNSEv0v0Z","colab_type":"code","colab":{}},"source":["# Copyright 2020 Google LLC\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUY2qA_f1jae","colab_type":"text"},"source":["# Overview\n","This Notebook aims to automate the analysis and SQL coding needed when creating an input data set for a propensity model. More specifically, it's designed for Binning and Dimensionality Reduction of Categorical Variables from the Google Analytics Big Query Export. \n","\n","The notebook will allow the user to pass standard and custom dimensions from Google Analytics Big Query Export into a function that will return a summary table of top values in each dimension as well as SQL code to be used when creating a propensity model's input dataset.\n","\n","## Dataset\n","This notebook is designed to work with categorical variables, both standard and cusotom dimensions, from the Google Analytics Big Query Export\n","\n","## Objective\n","The goal of the notebook is to speed up creation of the input dataset by automating summary tables and sql code for each categorical variable in the Google Analytics Big Query Export. \n","\n","This allows the user to quickly determine which value within each categorical variable should and shouln't be included in the input data set then easily port (copy/paste) the corresponding SQL code into thier SQL enviroment for creation of the input dataset.\n","\n","## Cost\n","This tutorial uses billable components of Google Cloud Platform (GCP):\n","\n","- Cloud AI Platform\n","- Cloud Storage\n","Learn about Cloud AI Platform pricing and Cloud Storage pricing, and use the Pricing Calculator to generate a cost estimate based on your projected usage.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ze4-nDLfK4pw","colab_type":"text"},"source":["### Set up your local development environment\n","\n","**If you are using Colab or AI Platform Notebooks**, your environment already meets\n","all the requirements to run this notebook. You can skip this step."]},{"cell_type":"markdown","metadata":{"id":"gCuSR8GkAgzl","colab_type":"text"},"source":["**Otherwise**, make sure your environment meets this notebook's requirements.\n","You need the following:\n","\n","* The Google Cloud SDK\n","* Git\n","* Python 3\n","* virtualenv\n","* Jupyter notebook running in a virtual environment with Python 3\n","\n","The Google Cloud guide to [Setting up a Python development\n","environment](https://cloud.google.com/python/setup) and the [Jupyter\n","installation guide](https://jupyter.org/install) provide detailed instructions\n","for meeting these requirements. The following steps provide a condensed set of\n","instructions:\n","\n","1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n","\n","2. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n","\n","3. [Install\n","   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n","   and create a virtual environment that uses Python 3.\n","\n","4. Activate that environment and run `pip install jupyter` in a shell to install\n","   Jupyter.\n","\n","5. Run `jupyter notebook` in a shell to launch Jupyter.\n","\n","6. Open this notebook in the Jupyter Notebook Dashboard."]},{"cell_type":"markdown","metadata":{"id":"BF1j6f9HApxa","colab_type":"text"},"source":["### Set up your GCP project\n","\n","**The following steps are required, regardless of your notebook environment.**\n","\n","1. [Select or create a GCP project.](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n","\n","2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n","\n","3. [Enable the AI Platform APIs and Compute Engine APIs.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component)\n","\n","4. Enter your project ID in the cell below. Then run the  cell to make sure the\n","Cloud SDK uses the right project for all the commands in this notebook.\n","\n","**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."]},{"cell_type":"code","metadata":{"id":"oM1iC_MfAts1","colab_type":"code","colab":{}},"source":["PROJECT_ID = \"\" #@param {type:\"string\"}\n","! gcloud config set project $PROJECT_ID"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dr--iN2kAylZ","colab_type":"text"},"source":["### Authenticate your GCP account\n","\n","**If you are using AI Platform Notebooks**, your environment is already\n","authenticated. Skip this step."]},{"cell_type":"markdown","metadata":{"id":"sBCra4QMA2wR","colab_type":"text"},"source":["**If you are using Colab**, run the cell below and follow the instructions\n","when prompted to authenticate your account via oAuth.\n","\n","**Otherwise**, follow these steps:\n","\n","1. In the GCP Console, go to the [**Create service account key**\n","   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n","\n","2. From the **Service account** drop-down list, select **New service account**.\n","\n","3. In the **Service account name** field, enter a name.\n","\n","4. From the **Role** drop-down list, select\n","   **Machine Learning Engine > AI Platform Admin** and\n","   **Cloud Storage > Storage Object Admin**.\n","\n","5. Click *Create*. A JSON file that contains your key downloads to your\n","local environment.\n","\n","6. Enter the path to your service account key as the\n","`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."]},{"cell_type":"code","metadata":{"id":"PyQmSRbKA8r-","colab_type":"code","colab":{}},"source":["import sys\n","\n","# If you are running this notebook in Colab, run this cell and follow the\n","# instructions to authenticate your GCP account. This provides access to your\n","# Cloud Storage bucket and lets you submit training jobs and prediction\n","# requests.\n","\n","if 'google.colab' in sys.modules:\n","  from google.colab import auth as google_auth\n","  google_auth.authenticate_user()\n","\n","# If you are running this notebook locally, replace the string below with the\n","# path to your service account key and run this cell to authenticate your GCP\n","# account.\n","else:\n","  %env GOOGLE_APPLICATION_CREDENTIALS ''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4WzZZYkCrCIh","colab_type":"text"},"source":["## Imports & Inputs"]},{"cell_type":"code","metadata":{"id":"qWoIeqtNrOAd","colab_type":"code","colab":{}},"source":["# Install sidetable because it is not pre-installed colab library\n","!pip install sidetable==0.6.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uzVm3pDFmmqU","colab_type":"text"},"source":["sidetable documentation: https://pbpython.com/sidetable.html"]},{"cell_type":"code","metadata":{"id":"MaXy5u4yrEI5","colab_type":"code","colab":{}},"source":["# Python Library Imports\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import pandas as pd\n","import re\n","import sidetable"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DjQOhuV8boor","colab_type":"text"},"source":["**Set Dataset Paramaters Below**"]},{"cell_type":"code","metadata":{"id":"xsnL3zU1tFBd","colab_type":"code","colab":{}},"source":["#@title Dataset Parameters\n","# Google Analytics Big Query Export Paramaters\n","project_id_billing = \"\" #@param {type:\"string\"}\n","dataset_id = \"bigquery-public-data.google_analytics_sample\" #@param {type:\"string\"}\n","table_id = \"ga_sessions_*\" #@param {type:\"string\"}\n","start_date = \"20170801\" #@param {type:\"string\"}\n","end_date = \"20170801\" #@param {type:\"string\"}\n","country = \"United States\" #@param {type:\"string\"}\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1vTJidIkahFG","colab_type":"text"},"source":["**Set Default Summary Table Paramaters Below**\n","This will set default values for the function. You can always specify different values within the function's arguments"]},{"cell_type":"code","metadata":{"id":"v4IIFu8jRcKm","colab_type":"code","colab":{}},"source":["#@title Summary Table Paramaters\n","#@markdown **Threshold**: Unique values that contribute to the top X%. Eliminates long-tail results\n","threshold = 95 #@param {type:\"slider\", min: 0, max: 100, step:5}\n","#@markdown **Max Rows**: Maximum number of rows to display. Helps when there are many results that fall under the threshold\n","max_rows = 10 #@param {type:\"integer\"}\n","#@markdown **Metric**: Calculations will be done based on Vists or Unique Users\n","metric = 'visits' #@param [\"visits\", \"user_cnt\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZwQ4Xq9iss5n","colab_type":"text"},"source":["## Stanard Dimension Function"]},{"cell_type":"markdown","metadata":{"id":"RXs4BtYRcXok","colab_type":"text"},"source":["Run the below cell to create the Feature Buckets function for standard dimensions"]},{"cell_type":"code","metadata":{"id":"AUKLPGOCFu_3","colab_type":"code","colab":{}},"source":["def featureBuckets(dimension_list, threshold=threshold, max_rows=max_rows):\n","\n","  for field in dimension_list:\n","    try:\n","      # Run SQL based on paramaters\n","      sql = f\"\"\"\n","            SELECT\n","              '{field}' as dimension,\n","              {field} as value,\n","              count(distinct clientId) as user_cnt,\n","              count(distinct concat(fullVisitorId, visitId)) as visits\n","            FROM `{dataset_id}.{table_id}` AS visits\n","              ,UNNEST(visits.hits) as hits\n","            WHERE \n","              _TABLE_SUFFIX BETWEEN '{start_date}' AND '{end_date}'\n","              AND geoNetwork.Country = '{country}'\n","            GROUP BY 1, 2\n","            \"\"\"\n","      \n","      # Create Frequnecy Distribution DataFrame based on threshold and metric\n","      df = pd.read_gbq(sql, project_id=project_id_billing, dialect='standard') # Create DataFrame from SQL statement\n","      df = df.stb.freq(['value'], value=metric, thresh=threshold) # Use SideTable to create frequncy distribtion\n","      df['dimension'] = field # Add columns specifying dimension\n","      display(df[['dimension','value', 'visits', 'percent', 'cumulative_visits', 'cumulative_percent']][:max_rows])\n","      print(\"\")\n","\n","      # Create SQL statement for Binary Variables of top values based from each dimension based on threshold and max_rows\n","      i = 1\n","      for index, row in df[:-1].iterrows():\n","        print(\"Max(CASE WHEN \",row['dimension'],\"='\",row['value'],\"' THEN 1 ELSE 0 END) AS \",\\\n","              re.sub('[\\\\\\\\/:*?\" <>|._()&,-]','',row['dimension']),'_',\\\n","              re.sub('[\\\\\\\\/:*?\" <>|._()&,-]','',row['value']),\",\",sep=\"\")\n","        if i == max_rows:\n","          break\n","        i += 1\n","      print(\"\")\n","    \n","    except:\n","      print('Could not run', field)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FVK5UAb51AXa","colab_type":"code","colab":{}},"source":["# Example of Output\n","featureBuckets(['device.browser'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I6na1sQrLBVD","colab_type":"text"},"source":["### Run Function"]},{"cell_type":"markdown","metadata":{"id":"Gv9w7h8Qczir","colab_type":"text"},"source":["- Pass lists of dimensions into the function to get summary tables and SQL code that you can copy/paste directly into your input data SQL code\n","\n","- Note: in each function you can also pass arguments for `max_rows` and `threshold`, so you don't have the always use the default paramaters set at the beginning of this notebook \n","\n","- Included already are standard dimensions for device, traffic source, page, geo, and ecommerce."]},{"cell_type":"markdown","metadata":{"id":"kHbHBSpTgi8V","colab_type":"text"},"source":["#### Device"]},{"cell_type":"code","metadata":{"id":"Ikap18jO2qeJ","colab_type":"code","colab":{}},"source":["device = [\n","          'device.browser',\n","          'device.browserSize',\n","          'device.browserVersion',\n","          'device.deviceCategory',\n","          'device.mobileDeviceInfo',\n","          'device.mobileDeviceMarketingName',\n","          'device.mobileDeviceModel',\n","          'device.mobileInputSelector',\n","          'device.operatingSystem',\n","          'device.operatingSystemVersion',\n","          'device.mobileDeviceBranding',\n","          'device.flashVersion',\n","          'device.javaEnabled',\n","          'device.language',\n","          'device.screenColors',\n","          'device.screenResolution'\n","          ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kssBxa1wFzKV","colab_type":"code","colab":{}},"source":["featureBuckets(device, threshold=95)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pSpYqY4Xgm9v","colab_type":"text"},"source":["#### Traffic Source"]},{"cell_type":"code","metadata":{"id":"DB81N02P_j9X","colab_type":"code","colab":{}},"source":["traffic_source = [\n","                  'trafficSource.adContent',\n","                  'trafficSource.campaign',\n","                  'trafficSource.campaignCode',\n","                  'trafficSource.isTrueDirect',\n","                  'trafficSource.keyword',\n","                  'trafficSource.medium',\n","                  'trafficSource.referralPath',\n","                  'trafficSource.source',\n","                  'channelGrouping'\n","                  ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"41hHZHidMEY3","colab_type":"code","colab":{}},"source":["featureBuckets(traffic_source, threshold=.90)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uYyyvPxYgrtL","colab_type":"text"},"source":["#### Geo\n"]},{"cell_type":"code","metadata":{"id":"tQPwLvX5_joL","colab_type":"code","colab":{}},"source":["geo = [\n","      'geoNetwork.subContinent',\n","      'geoNetwork.country',\n","      'geoNetwork.region',\n","      'geoNetwork.metro'\n","      'geoNetwork.city'\n","       ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e7u__zMMME8O","colab_type":"code","colab":{}},"source":["featureBuckets(geo, threshold=.90, max_rows=100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vxfJs_f3gzEL","colab_type":"text"},"source":["#### Page"]},{"cell_type":"code","metadata":{"id":"gu_YXqYQ_jbE","colab_type":"code","colab":{}},"source":["page = [\n","        'hits.page.pagePath',\n","        'hits.page.pagePathLevel1',\n","        'hits.page.pagePathLevel2',\n","        'hits.page.pagePathLevel3',\n","        'hits.page.pagePathLevel4',\n","        'hits.page.hostname',\n","        'hits.page.pageTitle',\n","        'hits.page.searchKeyword'\n","        ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"39VLq81ZMFfm","colab_type":"code","colab":{}},"source":["featureBuckets(page, threshold=.80)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MYzS0Huyg2hb","colab_type":"text"},"source":["#### Ecommerce"]},{"cell_type":"code","metadata":{"id":"cTLvrPrO_jN6","colab_type":"code","colab":{}},"source":["ecomm = [\n","         'eCommerceAction.action_type',\t\n","         'eCommerceAction.step',\t\n","         'eCommerceAction.option'\n","         ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-dyutTUu_jAX","colab_type":"code","colab":{}},"source":["featureBuckets(ecomm, threshold=.90)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mj8KjGiQSEz6","colab_type":"text"},"source":["## Custom Dimension Function"]},{"cell_type":"markdown","metadata":{"id":"7Z2QMLK8dl9U","colab_type":"text"},"source":["If you do not know your Custom Dimension Index values, run the below cell to return a list of Custom Dimensions Indexes used in your dataset"]},{"cell_type":"code","metadata":{"id":"4U5LERONTZg1","colab_type":"code","colab":{}},"source":["# Return all custome dimension indexes in list of values to pass to function\n","sql = f\"\"\"\n","      SELECT\n","        customDimensions.index\n","      FROM \n","        `{dataset_id}.{table_id}` AS visits\n","        ,UNNEST(visits.hits) as hits\n","        ,UNNEST(hits.customDimensions) as customDimensions\n","      WHERE \n","        _TABLE_SUFFIX BETWEEN '{start_date}' AND '{end_date}'\n","        AND geoNetwork.Country = '{country}'\n","      GROUP BY 1\n","      ORDER BY 1 ASC\n","      \"\"\"\n","\n","customDimension_list = list(pd.read_gbq(sql, project_id=project_id_billing, dialect='standard')['index'])\n","print(customDimension_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WTedG4gbdhQl"},"source":["Run the below cell to create the Feature Buckets function for custom dimensions"]},{"cell_type":"code","metadata":{"id":"_esfaLSR_in6","colab_type":"code","colab":{}},"source":["def CDfeatureBuckets(customDimension_list, threshold=threshold, max_rows=max_rows):\n","\n","  for field in list(customDimension_list):\n","    try:\n","      # Run SQL based on paramaters\n","      sql = f\"\"\"\n","            SELECT\n","              '{field}' as dimension,\n","              customDimensions.value as value,\n","              count(distinct clientId) as user_cnt,\n","              count(distinct concat(fullVisitorId, visitId)) as visits\n","            FROM `{dataset_id}.{table_id}` AS visits\n","              ,UNNEST(visits.hits) as hits\n","              ,UNNEST(hits.customDimensions) as customDimensions\n","            WHERE \n","              _TABLE_SUFFIX BETWEEN '{start_date}' AND '{end_date}'\n","              AND geoNetwork.Country = '{country}'\n","              AND customDimensions.index = {field}\n","            GROUP BY 1, 2\n","            \"\"\"\n","      \n","      # Create Frequnecy Distribution DataFrame based on threshold\n","      df = pd.read_gbq(sql, project_id=project_id_billing, dialect='standard') # Create DataFrame from SQL statement\n","      df = df.stb.freq(['value'], value=metric, thresh=threshold) # Use SideTable to create frequncy distribtion\n","      df['dimension'] = field # Add columns specifying dimension\n","      display(df[['dimension','value', 'visits', 'percent', 'cumulative_visits', 'cumulative_percent']][:max_rows])\n","      print(\"\")\n","\n","      # Create SQL statement for Binary Variables of top values based from each dimension based on threshold and max_rows\n","      i = 1\n","      for index, row in df[:-1].iterrows():\n","        print(\"Max(CASE WHEN customDimensions.index = \",row['dimension'],\" AND customDimensions.value = '\",row['value'],\"' THEN 1 ELSE 0 END) AS cd\",\\\n","              row['dimension'],'_',\\\n","              re.sub('[\\\\\\\\/:*?\" <>|._()&,-]','',row['value']),\",\",sep=\"\")\n","        if i == max_rows:\n","          break\n","        i += 1\n","      print(\"\")\n","    \n","    except:\n","      print('Could not run', field)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NsWsE2iKb1xT","colab_type":"text"},"source":["### Run Function"]},{"cell_type":"markdown","metadata":{"id":"A3-yCRzf0HKE","colab_type":"text"},"source":["- Pass lists of custom dimensions into the function to get summary tables and SQL code that you can copy/paste directly into your input data SQL code. You can use the `customDimension_list` created above or pass your own list of values\n","\n","- Note: in each function you can also pass arguments for `max_rows` and `threshold`, so you don't have the always use the default paramaters set at the beginning of this notebook "]},{"cell_type":"code","metadata":{"id":"8Q0rb69E_iaV","colab_type":"code","colab":{}},"source":["CDfeatureBuckets(customDimension_list)"],"execution_count":null,"outputs":[]}]}